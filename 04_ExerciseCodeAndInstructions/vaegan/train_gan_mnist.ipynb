{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79333d5a",
   "metadata": {},
   "source": [
    "# GAN\n",
    "Train a GAN on the MNIST handwritten digit dataset. \n",
    "\n",
    "This makes use of: our custom Keras model class defined in vaegan.gan.py, our\n",
    "class for loading the MNIST dataset defined in vaegan.data, and our custom Keras\n",
    "callback in vaegan.callbacks.\n",
    "\n",
    "A directory called 'output' will be created to save figures and the trained\n",
    "model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a44290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nEpochs=40  # orig, longer training\n",
    "nEpochs=3  # quick testing during development"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69f006d5",
   "metadata": {},
   "source": [
    "## 1. Import 3rd party libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2e797f6",
   "metadata": {},
   "source": [
    "## 2. Import our own classes (that we will complete together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c82b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our own classes \n",
    "from vaegan.data import MNIST\n",
    "from vaegan.callbacks import GenerateImages\n",
    "import vaegan.gan\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c10a105",
   "metadata": {},
   "source": [
    "## 3. Show some our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5af24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist yet.\n",
    "output_dir = './outputs/mnist_gan'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "# Instantiate the MNIST class containing our training data.\n",
    "data = MNIST()\n",
    "\n",
    "# Show some example images and their labels.\n",
    "data.show_example_images(os.path.join(output_dir, 'example_images.png'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4471becd",
   "metadata": {},
   "source": [
    "## 4. Construct the model using the python class you completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc5988",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyModule = reload(vaegan.gan)\n",
    "\n",
    "\n",
    "# Create the model. Note that we're using mostly the default arguments, but this is\n",
    "# where you might want to play around with different loss weights.\n",
    "tf.random.set_seed(1234)\n",
    "model = pyModule.GAN()\n",
    "\n",
    "# This step tells Keras to compute the explicit output shapes of each layer.\n",
    "# Otherwise, the layers will have dynamic/variable output shapes which is not\n",
    "# compatible with saving and loading.\n",
    "model.compute_output_shape((None, 32, 32, 1))\n",
    "model.discriminator.compute_output_shape((None, 32, 32, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0f7a76c",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> Correct model dimensions </span>\n",
    "    \n",
    "\n",
    "<span style=\"color:blue\"> === OVERALL MODEL ==== </span>\n",
    "\n",
    "```\n",
    "Model: \"gan\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "generator (Generator)        multiple                  1582465   \n",
    "_________________________________________________________________\n",
    "discriminator (Discriminator multiple                  50145     \n",
    "_________________________________________________________________\n",
    "gen_loss (Mean)              multiple                  2         \n",
    "_________________________________________________________________\n",
    "disc_loss (Mean)             multiple                  2         \n",
    "=================================================================\n",
    "Total params: 1,632,614\n",
    "Trainable params: 1,632,418\n",
    "Non-trainable params: 196\n",
    "_________________________________________________________________\n",
    "\n",
    "```\n",
    "<span style=\"color:blue\"> === GENERATOR SUBMODEL ====</span>\n",
    "```\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_11 (InputLayer)        [(None, 128)]             0         \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 8192)              1056768   \n",
    "_________________________________________________________________\n",
    "relu_dense (LeakyReLU)       (None, 8192)              0         \n",
    "_________________________________________________________________\n",
    "reshape (Reshape)            (None, 8, 8, 128)         0         \n",
    "_________________________________________________________________\n",
    "conv0 (Conv2DTranspose)      (None, 16, 16, 128)       262272    \n",
    "_________________________________________________________________\n",
    "relu0 (LeakyReLU)            (None, 16, 16, 128)       0         \n",
    "_________________________________________________________________\n",
    "conv1 (Conv2DTranspose)      (None, 32, 32, 128)       262272    \n",
    "_________________________________________________________________\n",
    "relu1 (LeakyReLU)            (None, 32, 32, 128)       0         \n",
    "_________________________________________________________________\n",
    "conv_out (Conv2D)            (None, 32, 32, 1)         1153      \n",
    "_________________________________________________________________\n",
    "sigmoid_out (Activation)     (None, 32, 32, 1)         0         \n",
    "=================================================================\n",
    "Total params: 1,582,465\n",
    "Trainable params: 1,582,465\n",
    "Non-trainable params: 0\n",
    "\n",
    "```\n",
    "<span style=\"color:blue\"> === DISCRIMINATOR SUBMODEL ====</span>\n",
    "```\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_12 (InputLayer)        [(None, 32, 32, 1)]       0         \n",
    "_________________________________________________________________\n",
    "conv0 (Conv2D)               (None, 32, 32, 32)        544       \n",
    "_________________________________________________________________\n",
    "bn0 (BatchNormalization)     (None, 32, 32, 32)        128       \n",
    "_________________________________________________________________\n",
    "relu0 (ReLU)                 (None, 32, 32, 32)        0         \n",
    "_________________________________________________________________\n",
    "conv1 (Conv2D)               (None, 16, 16, 64)        32832     \n",
    "_________________________________________________________________\n",
    "bn1 (BatchNormalization)     (None, 16, 16, 64)        256       \n",
    "_________________________________________________________________\n",
    "relu1 (ReLU)                 (None, 16, 16, 64)        0         \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 16384)             0         \n",
    "_________________________________________________________________\n",
    "dense_out (Dense)            (None, 1)                 16385     \n",
    "_________________________________________________________________\n",
    "sigmoid_out (Activation)     (None, 1)                 0         \n",
    "=================================================================\n",
    "Total params: 50,145\n",
    "Trainable params: 49,953\n",
    "Non-trainable params: 192\n",
    "\n",
    "```\n",
    "## 5. Now check your model's  dimensions against this list above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8e96c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== OVERALL MODEL ====\")\n",
    "model.summary()\n",
    "print(\"=== GENERATOR SUBMODEL ====\")\n",
    "gen_in = tf.keras.layers.Input(model.n_latent_dims)\n",
    "gen_out  = model.generator.call(gen_in) \n",
    "gen = tf.keras.Model(gen_in, gen_out) \n",
    "gen.summary()\n",
    "print(\"=== DISCRIMINATOR SUBMODEL ====\")\n",
    "disc_in = tf.keras.layers.Input(model.image_shape) \n",
    "disc_out  = model.discriminator.call(disc_in) \n",
    "disc = tf.keras.Model(disc_in, disc_out) \n",
    "disc.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfafbc40",
   "metadata": {},
   "source": [
    "## 6. Compile the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72444a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with an optimizer. The learning rate of the optimizer can be\n",
    "# specified here. Normally, this is also where you would select a loss function\n",
    "# and any metrics. However, our custom model defines the loss functions inside\n",
    "# its __init__ constructor, so we don't need to do that here. \n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001))\n",
    "\n",
    "# Instantiate our custom callback to save a few example reconstructions and\n",
    "# generated images after each epoch.\n",
    "save_images_callback = GenerateImages(output_dir=output_dir, \n",
    "                                      model=model,\n",
    "                                      n_generated_images=10,\n",
    "                                      n_latent_dims=model.n_latent_dims)\n",
    "\n",
    "# the number of latent dims is 128\n",
    "# model.build(input_shape=(60000, 32, 32, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d0b6e1e",
   "metadata": {},
   "source": [
    "## 7. Train (fit) the model on the data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model. Just like any off-the-shelf Keras model, we just call fit.\n",
    "# Under the hood, Keras will call the train_step method of our custom subclass\n",
    "# on each mini-batch and automatically loop through the training data. It will\n",
    "# take care of all the details, like converting numpy arrays to tensors, showing\n",
    "# a progress bar, and tracking the loss over the epochs.\n",
    "logs = model.fit(data.images_train,\n",
    "                 batch_size=128,\n",
    "                 epochs=nEpochs,\n",
    "                 callbacks=[save_images_callback])\n",
    "\n",
    "# 469 is the number of iterations/epoch..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7397f299",
   "metadata": {},
   "source": [
    "## 8. Training saves results to disk, now also plot training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ed9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training curves, which are stored in logs.history as a dict. Keys of\n",
    "# this dict are the metric names, while the corresponding values are arrays.\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for loss_name in ['gen_loss', 'disc_loss']:\n",
    "    loss_values = logs.history[loss_name]\n",
    "    x = np.arange(len(loss_values))\n",
    "    ax.plot(x, loss_values, label=loss_name)\n",
    "    \n",
    "ax.legend()\n",
    "ax.set_xlabel('Epoch')\n",
    "fig.savefig(os.path.join(output_dir, 'training_curves.png'), transparent=False)\n",
    "# fig.show()\n",
    "\n",
    "# Save the model \n",
    "model.save(os.path.join(output_dir, 'gan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403fa2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d40ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa71c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "276cb92f24b41b9f0d970b9341f6985e33d08dd20600905fc8ace2fed9003440"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
